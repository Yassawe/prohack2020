{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, hidden_layers):\n",
    "        hidden_layers.insert(0,77)\n",
    "        hidden_layers.append(1)\n",
    "        self.layers=hidden_layers\n",
    "        self.L=len(hidden_layers)\n",
    "        print(self.L)\n",
    "        \n",
    "        self.beta={}\n",
    "        self.gamma={}\n",
    "        self.W={}\n",
    "        self.b={}\n",
    "        \n",
    "        self.dbeta={}\n",
    "        self.dgamma={}\n",
    "        self.dW={}\n",
    "        self.db={}\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        for i in range(1, self.L):\n",
    "            self.W[i]=tf.Variable(tf.random.normal(shape=(self.layers[i],self.layers[i-1])))\n",
    "            self.b[i]=tf.Variable(tf.random.normal(shape=(self.layers[i],1)))\n",
    "            self.beta[i]=tf.Variable(tf.random.normal(shape=(1,1)))\n",
    "            self.gamma[i]=tf.Variable(tf.random.normal(shape=(1,1)))\n",
    "            \n",
    "    def forward_pass(self,A):\n",
    "        for i in range(1, self.L):\n",
    "            Z_norm, _ = tf.linalg.normalize(tf.matmul(self.W[i],A)+self.b[i], axis=1)\n",
    "            Z=self.gamma[i]*Z_norm+self.beta[i]\n",
    "            A=tf.nn.sigmoid(Z)\n",
    "        return A\n",
    "    \n",
    "    def compute_cost(self,Yhat,Y):\n",
    "        cost=tf.sqrt(tf.reduce_mean(tf.math.squared_difference(Yhat, Y))) #root mean square error cost function\n",
    "        return cost\n",
    "    \n",
    "    def evaluate_rmse(self, predicted, Y):\n",
    "        rmse=tf.sqrt(tf.reduce_mean(tf.math.squared_difference(predicted, Y))) #functions are same, because so far I use RMSE as cost func\n",
    "        return rmse\n",
    "    \n",
    "    def update_parameters(self,learning_rate, normalization_lr):\n",
    "            for i in range(1,self.L):\n",
    "                self.W[i].assign_sub(learning_rate*self.dW[i])\n",
    "                self.b[i].assign_sub(learning_rate*self.db[i])\n",
    "                self.beta[i].assign_sub(normalization_lr*self.dbeta[i])\n",
    "                self.gamma[i].assign_sub(normalization_lr*self.dgamma[i])\n",
    "    \n",
    "    def train_on_batch(self, X_batch, Y_batch, learning_rate, normalization_lr):\n",
    "        X_batch=tf.convert_to_tensor(X_batch, dtype=tf.float32)\n",
    "        Y_batch=tf.convert_to_tensor(Y_batch, dtype=tf.float32)\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            Yhat=self.forward_pass(X_batch)\n",
    "            cost=self.compute_cost(Yhat,Y_batch)\n",
    "        for j in range(1,self.L):\n",
    "            self.dW[j]=g.gradient(cost, self.W[j])\n",
    "            self.db[j]=g.gradient(cost, self.b[j])\n",
    "            self.dbeta[j]=g.gradient(cost, self.beta[j])\n",
    "            self.dgamma[j]=g.gradient(cost,self.gamma[j])\n",
    "        del g\n",
    "        self.update_parameters(learning_rate, normalization_lr)\n",
    "        return cost.numpy()\n",
    "        \n",
    "    def train(self, X, Y, epochs, batch_size, learning_rate, decay_rate, normalization_lr):\n",
    "        steps_per_epoch_non_floored=X.shape[1]/batch_size\n",
    "        steps_per_epoch=int(steps_per_epoch_non_floored)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            epoch_loss=0\n",
    "            learning_rate=learning_rate/(1+decay_rate*e)\n",
    "            for i in range(steps_per_epoch):\n",
    "                X_batch=X[:,i*batch_size:(i+1)*batch_size]\n",
    "                Y_batch=Y[:,i*batch_size:(i+1)*batch_size]\n",
    "                batch_loss=self.train_on_batch(X_batch, Y_batch, learning_rate, normalization_lr)\n",
    "                epoch_loss+=batch_loss\n",
    "            \n",
    "            if steps_per_epoch<steps_per_epoch_non_floored:\n",
    "                X_last_batch=X[:,(i+1)*batch_size:]\n",
    "                Y_last_batch=Y[:,(i+1)*batch_size:]\n",
    "                batch_loss=self.train_on_batch(X_last_batch, Y_last_batch, learning_rate, normalization_lr)\n",
    "                epoch_loss+=batch_loss\n",
    "            \n",
    "            if(e%100==0):\n",
    "                print(\"Loss on epoch {} is: \".format(e),epoch_loss/steps_per_epoch)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X=tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        prediction=self.forward_pass(X)\n",
    "        return prediction\n",
    "    \n",
    "    def upload_parameters(self,W,b,beta,gamma):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.beta=beta\n",
    "        self.gamma=gamma\n",
    "            \n",
    "    \n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#первый датасет Доры\n",
    "X_train = pd.read_csv('nonorm_negignore_knn3/x_knn_train_negative.csv', header=0).values[:, 1:].T\n",
    "Y_train = pd.read_csv('nonorm_negignore_knn3/y_knn_train_negative.csv', header=0).values[:, 1:].T\n",
    "X_test = pd.read_csv('nonorm_negignore_knn3/x_knn_test_negative.csv', header=0).values[:, 1:].T\n",
    "Y_test = pd.read_csv('nonorm_negignore_knn3/y_knn_test_negative.csv', header=0).values[:, 1:].T\n",
    "\n",
    "#второй датасет Доры\n",
    "X_train = pd.read_csv('norm_negignore_knn3/x_knn_train_negative_normalized.csv', header=0).values[:, 1:].T\n",
    "Y_train = pd.read_csv('norm_negignore_knn3/y_knn_train_negative_normalized.csv', header=0).values[:, 1:].T\n",
    "X_test = pd.read_csv('norm_negignore_knn3/x_knn_test_negative_normalized.csv', header=0).values[:, 1:].T\n",
    "Y_test = pd.read_csv('norm_negignore_knn3/y_knn_test_negative_normalized.csv', header=0).values[:, 1:].T\n",
    "\n",
    "#третий датасет Доры\n",
    "X_train = pd.read_csv('norm_negignore_knn3+mean/x_train_knn+mean.csv', header=0).values[:, 1:].T\n",
    "Y_train = pd.read_csv('norm_negignore_knn3+mean/y_train_knn+mean.csv', header=0).values[:, 1:].T\n",
    "X_test = pd.read_csv('norm_negignore_knn3+mean/x_test_knn+mean.csv', header=0).values[:, 1:].T\n",
    "Y_test = pd.read_csv('norm_negignore_knn3+mean/y_test_knn+mean.csv', header=0).values[:, 1:].T\n",
    "\"\"\"\n",
    "#интерполяция + knn\n",
    "X_train = pd.read_csv('datasets/norm_negignore_interpol_knn3/x_train_knn+interpol.csv', header=0).values[:, 1:].T\n",
    "Y_train = pd.read_csv('datasets/norm_negignore_interpol_knn3/y_train_knn+interpol.csv', header=0).values[:, 1:].T\n",
    "X_test = pd.read_csv('datasets/norm_negignore_interpol_knn3/x_test_knn+interpol.csv', header=0).values[:, 1:].T\n",
    "Y_test = pd.read_csv('datasets/norm_negignore_interpol_knn3/y_test_knn+interpol.csv', header=0).values[:, 1:].T\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#параметры модели\n",
    "\n",
    "hidden_layers=[70,50] #кол-во элементов это кол-во hidden layers, значение элемента это кол-во neurons в этом layer\n",
    "learning_rate=0.01\n",
    "decay_rate=0\n",
    "\n",
    "normalization_lr=0.001\n",
    "batch_size=512\n",
    "\n",
    "epochs=15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=NN(hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.train(X_train, Y_train, epochs, batch_size, learning_rate, decay_rate, normalization_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_train=network.predict(X_train)\n",
    "error_train=network.evaluate_rmse(predictions_train,Y_train)\n",
    "\n",
    "predictions_test=network.predict(X_test)\n",
    "error_test=network.evaluate_rmse(predictions_test,Y_test)\n",
    "\n",
    "print(\"Average error on train set: \", error_train.numpy())\n",
    "print(\"Average error on test set: \", error_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=tf.Variable(tf.random.normal(shape=(3,5)))\n",
    "a=tf.convert_to_tensor(a, dtype=tf.float32)\n",
    "print(a)\n",
    "a, norm=tf.linalg.normalize(a, axis=1)\n",
    "print(a)\n",
    "print(norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
